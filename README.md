# Geometrical-methods-in-Machine-Learning
Optimization theory has played a very important role in recent developments in machine learning. In this work, we focus on the development of the discrete variational geometry formalism that allows the introduction of numerical methods as the dynamical solutions of a discrete mechanical system. This variational description of time-dependent mechanics results in the conservation of the symplectic structure in the fibers of the phase space of a mechanical system. Through the concept of retraction and the left and right actions of the tangent and cotangent spaces on a Lie group, gradient-based optimization methods are introduced as a family of accelerated integrators on Lie groups. The novelty of these lies in the generalization of *Classical Momentum* (CM) and *Nesterov Accelerated Gradient* (NAG) methods on this extended case. Finally, the performance of these is numerically simulated in two examples. One corresponding to a very expensive function to minimize (Rosenbrock function) and a second case where practical applications are found to train a *deep network* that distinguishes between human actions, based on a simplified description of the human body.
